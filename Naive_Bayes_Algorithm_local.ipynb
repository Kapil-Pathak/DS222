{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kapil/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/kapil/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/kapil/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kapil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter                        #####IMPORTSRequired1#####\n",
    "import itertools\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import operator\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00020265579223632812\n"
     ]
    }
   ],
   "source": [
    "#doc=f.read() ############Required2############\n",
    "start=time.time()\n",
    "def text_processing(line1):\n",
    "    text=line1.split(' \\t')[1].rpartition(\"> \")[2]\n",
    "    text= \" \".join(text.split(\".\"))\n",
    "    text=text.lower()\n",
    "    string=[]\n",
    "    for words in text.split():\n",
    "        words = (\"\".join(e for e in words if e.isalpha()))\n",
    "        if not words in stop_words:\n",
    "            string.append(ps.stem(words))\n",
    "    return string\n",
    "end=time.time()\n",
    "#print(a)\n",
    "print(end-start)\n",
    "#print(message.split('@en .\\n')[1].split(' \\t')[1].rpartition(\"> \")[2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) ########Required3#######\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.82851767539978\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Ind_doc=defaultdict(list)                         ############THIS BLOCK CREATES LIST OF LISTRequired4##############\n",
    "big_doc=defaultdict(list)\n",
    "entire_corpus=[]\n",
    "i=0\n",
    "N_doc=0\n",
    "labels=[]\n",
    "start1=time.time()\n",
    "with open('verysmall_train.txt') as f:\n",
    "    lines=f.readlines()\n",
    "    lines=lines[3:]\n",
    "    N_doc+=1\n",
    "    for line in lines:\n",
    "        labels.append(line.split(' \\t')[0].split(','))\n",
    "        \n",
    "        for ctgr in (line.split(' \\t')[0].split(\",\")):\n",
    "            big_doc[ctgr].append(text_processing(line))\n",
    "            entire_corpus+=text_processing(line)\n",
    "end1=time.time()        \n",
    "print(end1-start1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=Counter(list(itertools.chain.from_iterable(labels)))######Required5#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53058\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "V=len(set(entire_corpus)) #######Required6#######\n",
    "print(V)      \n",
    "if 'power' in list(set(entire_corpus)):\n",
    "    print(\"yes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "############THIS BLOCK MERGES LIST OF LIST INTO SINGLE LISTRequired7##############\n",
    "set1=set(list(itertools.chain.from_iterable(labels)))\n",
    "bigdoc=defaultdict(list)\n",
    "print(len(set1))\n",
    "#print(len(Ind_doc[3].split(\",\")))\n",
    "for ctgr in (list(set1)):\n",
    "    for t in range(len(big_doc[ctgr])):\n",
    "        bigdoc[ctgr]+=big_doc[ctgr][t]\n",
    "#print(Counter(bigdoc['Articles_containing_video_clips']))\n",
    "#print(list(big_doc['Articles_containing_video_clips']))\n",
    "#print(list(big_doc['Articles_containing_video_clips'][2]+big_doc['Articles_containing_video_clips'][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########THIS GIVES PRIOR LOG FREQUENCIES FOR WHOLE CORPUSRequired8#############\n",
    "logprior=defaultdict(int)\n",
    "#logprior=math.log10(int(C.values())/N)\n",
    "for key, value in C.items():\n",
    "    prob=math.log10(value/N_doc)\n",
    "    logprior[key]=prob\n",
    "#print(logprior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_dict(n, type): #######Required9#######\n",
    "    if n == 1:\n",
    "        return defaultdict(type)\n",
    "    else:\n",
    "        return defaultdict(lambda: nested_dict(n-1, type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = nested_dict(2, list)        ########Required10#########\n",
    "loglikelihood = nested_dict(2, list)\n",
    "ctgr_count=defaultdict(list)\n",
    "for ctgr in list(set1):\n",
    "    g=Counter(bigdoc[ctgr])\n",
    "    ctgr_count[ctgr]=0\n",
    "    for word in g.keys():\n",
    "        count[ctgr][word]=g[word]\n",
    "        ctgr_count[ctgr]+=g[word]\n",
    "    for word in g.keys():      \n",
    "        loglikelihood[ctgr][word]=math.log10((count[ctgr][word]+1)/(ctgr_count[ctgr]+V))\n",
    "#print(count['Articles_containing_video_clips']['anim'])\n",
    "#print(loglikelihood['Articles_containing_video_clips']['anim'])\n",
    "#print(ctgr_count['Articles_containing_video_clips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11614.933624267578\n"
     ]
    }
   ],
   "source": [
    "sum_ctgr=defaultdict(float) ##########Required11#########\n",
    "success=0\n",
    "failure=0\n",
    "st=time.time()\n",
    "with open('verysmall_train.txt') as f1:\n",
    "    lines=f1.readlines()\n",
    "    for line in lines[:20]:\n",
    "        text_for_test=text_processing(line)\n",
    "        for ctgr in list(set1):\n",
    "            sum_ctgr[ctgr]=logprior[ctgr]\n",
    "            for word in list(text_for_test):\n",
    "                if word in list(set(entire_corpus)):\n",
    "                    sum_ctgr[ctgr]=sum_ctgr[ctgr]+int(loglikelihood[ctgr].get(word,'0'))\n",
    "        prediction=min(sum_ctgr.items(), key=operator.itemgetter(1))[0]\n",
    "        if prediction in list(line.split(' \\t')[0].split(',')):\n",
    "            success+=1\n",
    "        else:\n",
    "            failure+=1\n",
    "en=time.time()\n",
    "print(en - st)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "print(success/(success+failure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['American_comedy_films', 'American_drama_films', 'American_film_actresses', 'American_film_directors', 'American_films', 'American_male_film_actors', 'American_male_television_actors', 'American_military_personnel_of_World_War_II', 'American_people_of_Irish_descent', 'American_television_actresses', 'Articles_containing_video_clips', 'Association_football_defenders', 'Association_football_forwards', 'Association_football_goalkeepers', 'Association_football_midfielders', 'Asteroids_named_for_people', 'Australian_rules_footballers_from_Victoria_(Australia)', 'Black-and-white_films', 'Brazilian_footballers', 'British_films', 'Columbia_University_alumni', 'Deaths_from_myocardial_infarction', 'English-language_albums', 'English-language_films', 'English-language_journals', 'English-language_television_programming', 'English_cricketers', 'English_footballers', 'Fellows_of_the_Royal_Society', 'French_films', 'German_footballers', 'Guggenheim_Fellows', 'Harvard_University_alumni', 'Hindi-language_films', 'Indian_films', 'Insects_of_Europe', 'Italian_films', 'Italian_footballers', 'Main_Belt_asteroids', 'Major_League_Baseball_pitchers', 'Rivers_of_Romania', 'Russian_footballers', 'Scottish_footballers', 'Serie_A_players', 'The_Football_League_players', 'Villages_in_Turkey', 'Villages_in_the_Czech_Republic', 'Windows_games', 'Yale_University_alumni']\n",
      "English-language_journals\n",
      "English-language_journals\n"
     ]
    }
   ],
   "source": [
    "print(sorted(sum_ctgr))\n",
    "print(prediction)\n",
    "print(min(sum_ctgr.items(), key=operator.itemgetter(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apollo', '', 'spaceflight', 'land', 'first', 'human', 'moon', 'american', 'neil', 'armstrong', 'buzz', 'aldrin', 'juli', '', '', '', 'utc', 'armstrong', 'becam', 'first', 'step', 'onto', 'lunar', 'surfac', 'six', 'hour', 'later', 'juli', '', '', 'utc', 'armstrong', 'spent', 'two', 'half', 'hour', 'outsid', 'spacecraft', 'aldrin', 'slightli', 'less', 'togeth', 'collect', '', '', 'pound', '', '', 'kg', 'lunar', 'materi', 'return', 'earth', 'third', 'member', 'mission', 'michael', 'collin', 'pilot', 'command', 'spacecraft', 'alon', 'lunar', 'orbit', 'armstrong', 'aldrin', 'return', 'day', 'later', 'trip', 'back', 'earth', 'launch', 'saturn', 'v', 'rocket', 'kennedi', 'space', 'center', 'merritt', 'island', 'florida', 'juli', '', 'apollo', '', 'fifth', 'man', 'mission', 'nasa', 'apollo', 'program', 'apollo', 'spacecraft', 'three', 'part', 'command', 'modul', 'cm', 'cabin', 'three', 'astronaut', 'part', 'land', 'back', 'earth', 'servic', 'modul', 'sm', 'support', 'command', 'modul', 'propuls', 'electr', 'power', 'oxygen', 'water', 'lunar', 'modul', 'lm', 'land', 'moon', 'sent', 'toward', 'moon', 'saturn', 'vs', 'upper', 'stage', 'astronaut', 'separ', 'spacecraft', 'travel', 'three', 'day', 'enter', 'lunar', 'orbit', 'armstrong', 'aldrin', 'move', 'lunar', 'modul', 'land', 'sea', 'tranquil', 'stay', 'total', 'ubd', 'hour', 'lunar', 'surfac', 'lift', 'upper', 'part', 'lunar', 'modul', 'rejoin', 'collin', 'command', 'modul', 'return', 'earth', 'land', 'pacif', 'ocean', 'juli', '', 'broadcast', 'live', 'tv', 'worldwid', 'audienc', 'armstrong', 'step', 'onto', 'lunar', 'surfac', 'describ', 'event', 'one', 'small', 'step', 'man', 'one', 'giant', 'leap', 'mankind', '', 'apollo', '', 'effect', 'end', 'space', 'race', 'fulfil', 'nation', 'goal', 'propos', '', 'u', 'presid', 'john', 'f', 'kennedi', 'speech', 'u', 'congress', 'decad', 'land', 'man', 'moon', 'return', 'safe', 'earth', 'en']\n"
     ]
    }
   ],
   "source": [
    "with open('verysmall_test.txt') as f1:\n",
    "    lines=f1.readlines()\n",
    "    print(text_processing(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('verysmall_train.txt','r')                ######EXTRACT LABELS###### \n",
    "message = f.read()\n",
    "N=10495\n",
    "while(i!=N):\n",
    "    labels.append(message.split('@en .\\n')[i].split(' \\t')[0].split(','))\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ind_doc=defaultdict(list)                         ############THIS BLOCK CREATES LIST OF LIST##############\n",
    "big_doc=defaultdict(list)\n",
    "entire_corpus=[]\n",
    "i=0\n",
    "while(i!=N):\n",
    "    Ind_doc[i]=message.split('@en .\\n')[i].split(' \\t')[0]\n",
    "    for ctgr in (Ind_doc[i].split(\",\")):\n",
    "        big_doc[ctgr].append(text_processing(message,i))\n",
    "        entire_corpus+=text_processing(message,i)\n",
    "    i+=1\n",
    "\n",
    "#print(\"--------------------------------\")\n",
    "#print(logprior.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=text_processing(message,2)\n",
    "#print(C)\n",
    "sum=0\n",
    "g=Counter(bigdoc['Articles_containing_video_clips'])\n",
    "print(g['anim'])\n",
    "for k,v in g.items():\n",
    "#    print(k,v)\n",
    "    sum+=v\n",
    "#file = open(\"testfile.txt\",\"w\") \n",
    "#file.write(str(Counter(bigdoc['Articles_containing_video_clips'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('verysmall_test.txt','r')\n",
    "test_text=f1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for ctgr in list(set1):\n",
    "    sum_ctgr[ctgr]=logprior[ctgr]\n",
    "    text_for_test=text_processing(test_text,k)\n",
    "    for word in list(text_for_test):\n",
    "        if word in list(set(entire_corpus)):\n",
    "            sum_ctgr[ctgr]=sum_ctgr[ctgr]+int(loglikelihood[ctgr].get(word,'0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_text.split('@en .\\n')[6].split(' \\t')[1].rpartition(\"> \")[2])\n",
    "print(test_text.split('@en .\\n')[6].split(' \\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loglikelihood['Indian_films'].get('clay',0))\n",
    "print(type(loglikelihood['Indian_films'].get('clay',0)))\n",
    "print(sum_ctgr['English-language_television_programming'])\n",
    "print(min(sum_ctgr.items(), key=operator.itemgetter(1))[0])\n",
    "print(sum_ctgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_for_test=text_processing(test_text,5)\n",
    "print(text_for_test)\n",
    "print(sorted(sum_ctgr.items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
